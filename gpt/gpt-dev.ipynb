{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "- Attention is All You Need paper: https://arxiv.org/abs/1706.03762\n",
    "- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165\n",
    "- GPU: https://lambdalabs.com/ \n",
    "- Data: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "- Video: https://www.youtube.com/watch?v=kCc8FmEb1nY [26:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1089k  100 1089k    0     0  6069k      0 --:--:-- --:--:-- --:--:-- 6119k\n"
     ]
    }
   ],
   "source": [
    "# download the data set \n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# read the data set \n",
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(len(text)) # len of the text\n",
    "print(text[:100]) # print the first 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# get all unique characters in the text\n",
    "char = sorted(set(text))\n",
    "vocab_size = len(char)\n",
    "print(vocab_size)\n",
    "print(''.join(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# building a character level encoding by a mapping from unique characters to integers \n",
    "\"\"\" \n",
    "stoi = {}  # Dictionary to store the index of each character in char list\n",
    "\n",
    "for i in range(len(char)):\n",
    "    ch = char[i]\n",
    "    stoi[ch] = i\n",
    "\"\"\"\n",
    "stoi = {ch:i for i,ch in enumerate(char)} # stores the index of each character in char\n",
    "itos = {i:ch for i,ch in enumerate(char)} # stores the character of each integer in char\n",
    "\n",
    "# string to integer list\n",
    "def encode(s:str) -> list:\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for c in s:\n",
    "        result.append(stoi[c])\n",
    "    return result\n",
    "    \"\"\"\n",
    "    return [stoi[c] for c in s] \n",
    "\n",
    "def decode(s:list) -> str:\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in s:\n",
    "        result.append(itos[i])\n",
    "    return ''.join(result)\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in s])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire dataset and store it in toruch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to train and validation sets\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n] # training data\n",
    "val_data = data[n:] # validation data; wil be used to get a sense of the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) then target is 47\n",
      "When input is tensor([18, 47]) then target is 56\n",
      "When input is tensor([18, 47, 56]) then target is 57\n",
      "When input is tensor([18, 47, 56, 57]) then target is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) then target is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) then target is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) then target is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then target is 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8  # length of one block of data; what is the maximum context length for predictions\n",
    "# understanding the block_size\n",
    "x = train_data[0:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"When input is {context} then target is {target}\")\n",
    "# helps the model predict everything up to block_size \n",
    "# as the transformer will never receive more than block_size at a time\n",
    "# keeps gpu busy as we can process multiple chunks of data in parallel\n",
    "# those chunks are completely independent of each other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 8])\n",
      " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "target shape: torch.Size([4, 8])\n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # how many independent sequences we want to process in parallel\n",
    "torch.manual_seed(1337) # will be used to sample random chunks of data\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # random starting indices for each chunk of data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack the 1d chunks as rows, 4 by 8 tensor\n",
    "    # each row is a chunk of the training set \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # stack the chunks shifted by 1\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"input shape: {xb.shape}\\n\", xb) # contains 32 examples (4*8) which are completely idenepndent\n",
    "# as each row has 8 examples  \n",
    "print(f\"target shape: {yb.shape}\\n\", yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 65], got [4, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[39mreturn\u001b[39;00m logits, loss\n\u001b[0;32m     33\u001b[0m m \u001b[39m=\u001b[39m BigramLangModel(vocab_size)\n\u001b[1;32m---> 34\u001b[0m logits, loss \u001b[39m=\u001b[39m m(xb, yb)\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(logits\u001b[39m.\u001b[39mshape, loss) \u001b[39m# (B, T, C) = (4, 8, 65)\u001b[39;00m\n",
      "File \u001b[1;32md:\\LLMS\\llm-testing\\llm-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[23], line 25\u001b[0m, in \u001b[0;36mBigramLangModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     19\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding_table(idx) \u001b[39m# (B, T, C) \u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# pytorch will arrange it to batch by time by channel \u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# batch is 4, time is 8 and C is vocab size which is 65\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# logits is the score for the next charecter \u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# we are predicting what comes next based on the indivisual identity of a single token \u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# we evaluate the loss\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(logits, targets) \u001b[39m# loss is the cross entropy between predictions and targets \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m# measures the quality of the logits with respect to the targets  \u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m# we have the identity of the next character so how well we are predicting the next character \u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m# the correct dimension of logits depeending on whatever the target is should have \u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# a very high number and all other dimensions should have a very low number\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[1;32md:\\LLMS\\llm-testing\\llm-venv\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [4, 65], got [4, 8]"
     ]
    }
   ],
   "source": [
    "# we are gonna start with the simplest possible model which is the bigram language model\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLangModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # each token directly reads off the logits of the next token from a lookup table \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # when we pass idx every integer in our input is going to refer to the embedding table\n",
    "        # and its going to pluck out a row of that embedding table corresponding to that index\n",
    "        #  \n",
    "        # idx and targets are both (B, T) tesors of integers \n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) \n",
    "        # pytorch will arrange it to batch by time by channel \n",
    "        # batch is 4, time is 8 and C is vocab size which is 65\n",
    "        # logits is the score for the next charecter \n",
    "        # we are predicting what comes next based on the indivisual identity of a single token \n",
    "        # we evaluate the loss\n",
    "        loss = F.cross_entropy(logits, targets) \n",
    "        # loss is the cross entropy between predictions and targets \n",
    "        # measures the quality of the logits with respect to the targets  \n",
    "        # we have the identity of the next character so how well we are predicting the next character \n",
    "        # the correct dimension of logits depeending on whatever the target is should have \n",
    "        # a very high number and all other dimensions should have a very low number\n",
    "        # cross_entropy needs a (B, C, T)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "m = BigramLangModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, loss) # (B, T, C) = (4, 8, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
